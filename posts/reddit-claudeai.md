# I built a memory system that survives Claude's context compaction — 761K memories, $45 total

Context compaction is the biggest pain point with long Claude sessions. I've seen multiple threads here about it — your agent loses track of decisions, debugging context, and nuance every time it compacts. The summary keeps the facts but loses the *why*.

I've been running autonomous Claude agents 24/7 for months and finally built a solution that eliminates compaction amnesia. Open-sourcing it today.

## How it works

**Before** compaction fires, the system intercepts the full context and fires 3 parallel calls to a fast/cheap model (Cerebras Llama 70B at ~$0.10/MTok, ~2000 tok/s) to extract:

1. **Facts & Entities** — names, dates, configs, URLs
2. **Decisions & Rationale** — what was chosen, what was rejected, *why*
3. **Skills & Lessons** — debugging techniques, workarounds, patterns

Each extraction gets an importance score (1-10). Memories scoring 7+ are committed to a vector database (Qdrant) with automatic deduplication.

**Cost: ~$0.02 per compaction event.** Not per session — per *compaction event*.

## Results after 3 months:

- 761K+ persistent memories in vector DB
- 16ms retrieval latency
- Zero context amnesia across 2,500+ compaction events
- Decision recall: 23% → 89% after compaction
- Total cost for 3 months of memory extraction: $45

The agent now recalls decisions, configurations, and debugging sessions from months ago — even after hundreds of compaction cycles.

## For OpenClaw users:

This integrates directly with OpenClaw's `preCompactionFlush` hook. Set it in your config and the memory rescue fires automatically before every compaction.

## For Claude.ai / Claude Code users:

The same technique works if you have any pre-compaction hook or can detect when context is getting high. The key principle is: **offload memory to a cheap model + vector DB before the expensive model's context gets wiped.**

**GitHub: [jcartu/cartu-method](https://github.com/jcartu/cartu-method)**

Python package with Qdrant, ChromaDB, and Pinecone backends. Works with any OpenAI-compatible fast inference endpoint.

Would love to hear if others have tried similar approaches. The compaction problem is solvable — we just need to stop treating it as inevitable.
