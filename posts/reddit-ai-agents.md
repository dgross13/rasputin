# The Cartu Method: Zero-loss context compaction for long-running AI agents ($0.02/event, 761K persistent memories)

I keep seeing posts here about agents "forgetting" things after context compaction. It's the #1 pain point with long-running agents — your agent works perfectly for hours, then context gets summarized and suddenly it can't recall why you chose Postgres over Mongo, or how you fixed that auth bug.

After months of running autonomous agents 24/7, this solution has been that's been working in production for 3 months with zero amnesia. Open-sourcing it today.

## TL;DR

Before context compaction fires, intercept the full context and fan out 3 parallel calls to fast/cheap models (Cerebras, Groq, or local Ollama) to extract and preserve critical memories in a vector database. Cost: ~$0.02 per compaction event.

## Why it works

The key insight is **separation of concerns**:
- Your **expensive model** (Opus, GPT-5) does reasoning and task execution
- Your **cheap fast model** (Cerebras Llama 70B at $0.10/MTok) does memory extraction
- Your **vector DB** (Qdrant, ChromaDB) does persistent storage and retrieval

You're not asking the reasoning model to also remember everything. You're offloading memory management to purpose-built, cost-effective infrastructure.

## Three extraction perspectives (run in parallel):

1. **Facts & Entities** — names, dates, configs, URLs, technical specs
2. **Decisions & Rationale** — what was chosen, what was rejected, and *why*
3. **Skills & Lessons** — debugging techniques, workarounds, patterns that worked

Each gets an importance score (1-10). Only memories scoring 7+ get committed. Automatic deduplication prevents bloat.

## Production results (3 months, dual-GPU server):

- 761K+ memories committed
- 16ms average retrieval
- $45 total cost
- Decision recall went from 23% → 89% after compaction
- Agent recalls conversations from months ago

## Works with:

- **Any agent framework** — OpenClaw, LangChain, CrewAI, custom
- **Any vector DB** — Qdrant, ChromaDB, Pinecone
- **Any fast model** — Cerebras, Groq, Ollama, OpenRouter

**GitHub: [jcartu/cartu-method](https://github.com/jcartu/cartu-method)**

The method is complementary to MemRL (arXiv:2601.03192) — Cartu solves *what gets remembered*, MemRL solves *what gets retrieved*. Combined = zero-loss memory + utility-aware retrieval.

Anyone else been dealing with this problem? Curious what approaches others have tried.
